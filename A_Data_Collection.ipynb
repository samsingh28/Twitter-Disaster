{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A. Data Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import bs4 \n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IMPORTANT: You need to download your own selenium driver here: http://chromedriver.chromium.org/downloads. Then, you need to make sure the file path goes to the right place in the 'get_tweets' function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convertimg to datetime\n",
    "def get_date(x): \n",
    "    return pd.to_datetime(x)\n",
    "    \n",
    "# getting tweets for each query - reset the number of scrolls and radius\n",
    "def get_tweets(city, state, since, until, radius=15, scrolls=30):\n",
    "    browser = webdriver.Chrome(executable_path='./chromedriver') # change to your driver\n",
    "    stem = \"https://twitter.com/search?l=&q=\"\n",
    "    URL = '{}near:%22{}%2C%20{}%22%20within:{}mi%20since:{}%20until:{}&src=typd'.format(stem, city, state, radius, since, until)\n",
    "    print(URL) # prints url as it's scraping it\n",
    "    browser.get(URL) # setting up scraper\n",
    "    body = browser.find_element_by_tag_name('body')\n",
    "    mylist = []\n",
    "    for i in range(scrolls): # scrolling based on the number of scrolls\n",
    "        body.send_keys(Keys.PAGE_DOWN)\n",
    "        try:\n",
    "            time.sleep(.1)\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        page_source = browser.page_source\n",
    "        \n",
    "        soup = bs4.BeautifulSoup(page_source, 'lxml') # setting up BeautifulSoup\n",
    "        \n",
    "        for row in soup.find_all('li', {'data-item-type': 'tweet'}): # setting up fields to scrape\n",
    "            tweet = {}\n",
    "            try:\n",
    "                tweet['time'] = row.find('a', {'class': 'tweet-timestamp js-permalink js-nav js-tooltip'}).attrs['title']\n",
    "            except:\n",
    "                pass\n",
    "            tweet['text'] = row.find('p').text\n",
    "            tweet['ID'] = row.attrs['id'].strip('stream-item-tweet-')\n",
    "            mylist.append(tweet)\n",
    "    \n",
    "    # setting list as dataframe\n",
    "    mydf = pd.DataFrame(mylist)\n",
    "    \n",
    "    browser.close() # closing browser after scrape\n",
    "    return mydf\n",
    "\n",
    "# function to iterate through rows in a dataframe for searching Twitter\n",
    "def iterate_disasters(df):\n",
    "    all_dfs = []\n",
    "    for row in df.index:\n",
    "        city = df.loc[row, 'declaredCountyArea']\n",
    "        state = df.loc[row, 'state']\n",
    "        disaster_begin = df.loc[row, 'incidentBeginDate']\n",
    "        before_disaster = (pd.to_datetime(disaster_begin) - pd.Timedelta(days=2)).strftime('%Y-%m-%d')\n",
    "        before_disaster_end = (pd.to_datetime(disaster_begin) - pd.Timedelta(days=1)).strftime('%Y-%m-%d')\n",
    "        after_disaster = (pd.to_datetime(disaster_begin) + pd.Timedelta(days=1)).strftime('%Y-%m-%d')\n",
    "        \n",
    "        # running 'get tweets' function for before disaster\n",
    "        tweets_disaster_before = get_tweets(city, state, before_disaster, before_disaster_end)\n",
    "        \n",
    "        # setting additional vars for each row\n",
    "        tweets_disaster_before['disaster_num'] = df.loc[row, 'disasterNumber'] \n",
    "        tweets_disaster_before['disaster_type'] = df.loc[row, 'title']\n",
    "        tweets_disaster_before['disaster_declared'] = df.loc[row, 'declarationDate']\n",
    "        tweets_disaster_before['disaster_started'] = df.loc[row, 'incidentBeginDate']\n",
    "        tweets_disaster_before['location'] = df.loc[row, 'declaredCountyArea']\n",
    "        tweets_disaster_before['disaster_happened'] = '0'\n",
    "        \n",
    "        all_dfs.append(tweets_disaster_before) # adding to list\n",
    "        \n",
    "        # running get tweets function for after disaster\n",
    "        tweets_disaster_after = get_tweets(city, state, disaster_begin, after_disaster)\n",
    "   \n",
    "        # setting additional vars for each row\n",
    "        tweets_disaster_after['disaster_num'] = df.loc[row, 'disasterNumber'] \n",
    "        tweets_disaster_after['disaster_type'] = df.loc[row, 'title']\n",
    "        tweets_disaster_after['disaster_declared'] = df.loc[row, 'declarationDate']\n",
    "        tweets_disaster_after['disaster_started'] = df.loc[row, 'incidentBeginDate']\n",
    "        tweets_disaster_after['location'] = df.loc[row, 'declaredCountyArea']\n",
    "        tweets_disaster_after['disaster_happened'] = '1'\n",
    "        \n",
    "        # adding each df to a csv\n",
    "        all_dfs.append(tweets_disaster_after) # instantiating empty list\n",
    "    \n",
    "    return all_dfs\n",
    "\n",
    "# Wrap function - this is the one to call \n",
    "def tweets_to_csv(df, csv_name):\n",
    "    X = iterate_disasters(df)\n",
    "    full = pd.concat(X, sort=False)\n",
    "    # changing time to timeseries\n",
    "    full['time'] = full['time'].str.replace(r'[-]', '').apply(get_date)\n",
    "    full.drop_duplicates(subset = ['ID', 'text'], inplace = True)\n",
    "    # saving to csv\n",
    "    full.to_csv(csv_name) # RENAME THIS before pushing it to github to avoid confusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### read in your csv\n",
    "to_scrape = pd.read_csv('./disasters')\n",
    "## what we used to filter by disaster type - OPTIONAL\n",
    "to_scrape_filtered = disasters[disasters['disasterType'] == 'DR'] # DR = major disaster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# breaking up the csv to scrape into parts (recommended)\n",
    "part = [[10, 110], [110, 210], [210, 310], [310, 410], [410, 510], [510, 547]]\n",
    "\n",
    "for i in part:\n",
    "    suffix = str(i[0]) + '_' + str(i[1])\n",
    "    csv_name = './disasters' + suffix + '.csv'\n",
    "    tweets_to_csv(to_scrape_filtered[i[0]:i[1]], csv_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you could also just run the scrape function on the whole csv:\n",
    "tweets_to_csv(to_scrape_filtered)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
